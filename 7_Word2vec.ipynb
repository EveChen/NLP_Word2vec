{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes\n",
    "#1. Word2vec can only use Python3 because Py2 have the ASC code error\n",
    "#2. Therefore, use \"source activate nlp3\" and then \"jupyter notebook\"\n",
    "\n",
    "# Word2Vec\n",
    "#1. It's a kind of method which is faster than some deep learning algorithms\n",
    "#2. Do not need labels, it can generated labels automatically based on relationships between words.\n",
    "#3. Because it cares relationships, don't remove stop words and numbers.\n",
    "#4. 關係性的比重如何算: Attemp1 Vector Average, Attemp2 Clustering\n",
    "\n",
    "# Trial1 - 5/29\n",
    "#1. 萃取出500個具有關係性的單詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000 50000\n",
      "\n",
      "\n",
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
      "\n",
      "\n",
      "Index(['id', 'sentiment', 'review'], dtype='object') Index(['id', 'review'], dtype='object') Index(['id', 'review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "test = pd.read_csv(\"testData.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "\n",
    "# print\n",
    "print(train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size)\n",
    "print(\"\\n\")\n",
    "print(train[\"review\"][0])\n",
    "print(\"\\n\")\n",
    "print(train.columns, test.columns, unlabeled_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean data (remove html, non-letters, lower case, split)\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review):\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'going',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'odd',\n",
       " 'documentary']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "list_1 = review_to_wordlist(train[\"review\"][0])\n",
    "list_1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Punkt tokenizer to split paragraphs to sentences\n",
    "# Above \"review_to_wordlist\" function is to split sentences into wordlist\n",
    "# paragraphs -> sentences -> wordlists 讓每句sentence都有自己的wordlist\n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review, tokenizer):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for i in raw_sentences:\n",
    "        if len(i) > 0:\n",
    "            sentences.append(review_to_wordlist(i))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again.', 'Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent.']\n",
      "\n",
      "\n",
      "[['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker'], ['maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent']]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "list_2 = tokenizer.tokenize(train[\"review\"][0].strip())\n",
    "print(list_2[0:2])\n",
    "print(\"\\n\")\n",
    "list_3 = review_to_sentences(train[\"review\"][0], tokenizer)\n",
    "print(list_3[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Start to make paragraph into list of words\n",
    "sentences = []\n",
    "for i in train[\"review\"]:\n",
    "    sentences += review_to_sentences(i, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266551\n",
      "\n",
      "\n",
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker']\n",
      "\n",
      "\n",
      "['maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(\"\\n\")\n",
    "print(sentences[0])\n",
    "print(\"\\n\")\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-29 10:47:45,567 : INFO : collecting all words and their counts\n",
      "2018-05-29 10:47:45,569 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-29 10:47:45,688 : INFO : PROGRESS: at sentence #10000, processed 114931 words, keeping 17627 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-29 10:47:45,768 : INFO : PROGRESS: at sentence #20000, processed 228988 words, keeping 24797 word types\n",
      "2018-05-29 10:47:45,854 : INFO : PROGRESS: at sentence #30000, processed 339533 words, keeping 29883 word types\n",
      "2018-05-29 10:47:45,920 : INFO : PROGRESS: at sentence #40000, processed 453983 words, keeping 34196 word types\n",
      "2018-05-29 10:47:45,987 : INFO : PROGRESS: at sentence #50000, processed 565006 words, keeping 37609 word types\n",
      "2018-05-29 10:47:46,057 : INFO : PROGRESS: at sentence #60000, processed 676637 words, keeping 40571 word types\n",
      "2018-05-29 10:47:46,128 : INFO : PROGRESS: at sentence #70000, processed 789005 words, keeping 43180 word types\n",
      "2018-05-29 10:47:46,196 : INFO : PROGRESS: at sentence #80000, processed 899771 words, keeping 45561 word types\n",
      "2018-05-29 10:47:46,259 : INFO : PROGRESS: at sentence #90000, processed 1013453 words, keeping 47982 word types\n",
      "2018-05-29 10:47:46,335 : INFO : PROGRESS: at sentence #100000, processed 1125135 words, keeping 50054 word types\n",
      "2018-05-29 10:47:46,386 : INFO : PROGRESS: at sentence #110000, processed 1236261 words, keeping 51928 word types\n",
      "2018-05-29 10:47:46,467 : INFO : PROGRESS: at sentence #120000, processed 1348541 words, keeping 53966 word types\n",
      "2018-05-29 10:47:46,529 : INFO : PROGRESS: at sentence #130000, processed 1461911 words, keeping 55694 word types\n",
      "2018-05-29 10:47:46,590 : INFO : PROGRESS: at sentence #140000, processed 1568503 words, keeping 57193 word types\n",
      "2018-05-29 10:47:46,659 : INFO : PROGRESS: at sentence #150000, processed 1682622 words, keeping 58902 word types\n",
      "2018-05-29 10:47:46,704 : INFO : PROGRESS: at sentence #160000, processed 1794988 words, keeping 60464 word types\n",
      "2018-05-29 10:47:46,779 : INFO : PROGRESS: at sentence #170000, processed 1907744 words, keeping 61924 word types\n",
      "2018-05-29 10:47:46,848 : INFO : PROGRESS: at sentence #180000, processed 2018412 words, keeping 63343 word types\n",
      "2018-05-29 10:47:46,921 : INFO : PROGRESS: at sentence #190000, processed 2131820 words, keeping 64641 word types\n",
      "2018-05-29 10:47:46,994 : INFO : PROGRESS: at sentence #200000, processed 2245187 words, keeping 65934 word types\n",
      "2018-05-29 10:47:47,073 : INFO : PROGRESS: at sentence #210000, processed 2357380 words, keeping 67237 word types\n",
      "2018-05-29 10:47:47,271 : INFO : PROGRESS: at sentence #220000, processed 2470883 words, keeping 68544 word types\n",
      "2018-05-29 10:47:47,355 : INFO : PROGRESS: at sentence #230000, processed 2582936 words, keeping 69805 word types\n",
      "2018-05-29 10:47:47,408 : INFO : PROGRESS: at sentence #240000, processed 2697681 words, keeping 71014 word types\n",
      "2018-05-29 10:47:47,544 : INFO : PROGRESS: at sentence #250000, processed 2805851 words, keeping 72198 word types\n",
      "2018-05-29 10:47:47,605 : INFO : PROGRESS: at sentence #260000, processed 2916657 words, keeping 73325 word types\n",
      "2018-05-29 10:47:47,646 : INFO : collected 74065 word types from a corpus of 2988095 raw words and 266551 sentences\n",
      "2018-05-29 10:47:47,648 : INFO : Loading a fresh vocabulary\n",
      "2018-05-29 10:47:47,869 : INFO : min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2018-05-29 10:47:47,870 : INFO : min_count=40 leaves 2627279 word corpus (87% of original 2988095, drops 360816)\n",
      "2018-05-29 10:47:47,935 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2018-05-29 10:47:47,942 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2018-05-29 10:47:47,944 : INFO : downsampling leaves estimated 2494390 word corpus (94.9% of prior 2627279)\n",
      "2018-05-29 10:47:47,999 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2018-05-29 10:47:48,001 : INFO : resetting layer weights\n",
      "2018-05-29 10:47:48,239 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-05-29 10:47:49,277 : INFO : EPOCH 1 - PROGRESS: at 20.24% examples, 496079 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:47:50,285 : INFO : EPOCH 1 - PROGRESS: at 41.36% examples, 508535 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:47:51,304 : INFO : EPOCH 1 - PROGRESS: at 62.14% examples, 508262 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:47:52,306 : INFO : EPOCH 1 - PROGRESS: at 81.12% examples, 500002 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-29 10:47:53,324 : INFO : EPOCH 1 - PROGRESS: at 97.90% examples, 481886 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:47:53,354 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-29 10:47:53,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-29 10:47:53,392 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-29 10:47:53,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-29 10:47:53,404 : INFO : EPOCH - 1 : training on 2988095 raw words (2493912 effective words) took 5.2s, 484238 effective words/s\n",
      "2018-05-29 10:47:54,418 : INFO : EPOCH 2 - PROGRESS: at 17.49% examples, 440769 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:47:55,430 : INFO : EPOCH 2 - PROGRESS: at 34.30% examples, 426352 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:47:56,433 : INFO : EPOCH 2 - PROGRESS: at 53.82% examples, 445031 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:47:57,434 : INFO : EPOCH 2 - PROGRESS: at 74.15% examples, 461011 words/s, in_qsize 8, out_qsize 1\n",
      "2018-05-29 10:47:58,440 : INFO : EPOCH 2 - PROGRESS: at 94.56% examples, 469911 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:47:58,648 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-29 10:47:58,655 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-29 10:47:58,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-29 10:47:58,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-29 10:47:58,684 : INFO : EPOCH - 2 : training on 2988095 raw words (2494201 effective words) took 5.3s, 473593 effective words/s\n",
      "2018-05-29 10:47:59,701 : INFO : EPOCH 3 - PROGRESS: at 19.91% examples, 498480 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:48:00,722 : INFO : EPOCH 3 - PROGRESS: at 42.03% examples, 519375 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:48:01,725 : INFO : EPOCH 3 - PROGRESS: at 62.18% examples, 512667 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:48:02,739 : INFO : EPOCH 3 - PROGRESS: at 81.76% examples, 505965 words/s, in_qsize 7, out_qsize 1\n",
      "2018-05-29 10:48:03,761 : INFO : EPOCH 3 - PROGRESS: at 91.08% examples, 450066 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:48:04,295 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-29 10:48:04,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-29 10:48:04,353 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-29 10:48:04,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-29 10:48:04,358 : INFO : EPOCH - 3 : training on 2988095 raw words (2495105 effective words) took 5.7s, 440982 effective words/s\n",
      "2018-05-29 10:48:05,380 : INFO : EPOCH 4 - PROGRESS: at 16.86% examples, 423746 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-29 10:48:06,387 : INFO : EPOCH 4 - PROGRESS: at 38.01% examples, 472899 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:48:07,387 : INFO : EPOCH 4 - PROGRESS: at 59.48% examples, 493017 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:48:08,394 : INFO : EPOCH 4 - PROGRESS: at 79.44% examples, 494263 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-29 10:48:09,339 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-29 10:48:09,343 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-29 10:48:09,360 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-29 10:48:09,373 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-29 10:48:09,375 : INFO : EPOCH - 4 : training on 2988095 raw words (2494179 effective words) took 5.0s, 499189 effective words/s\n",
      "2018-05-29 10:48:10,390 : INFO : EPOCH 5 - PROGRESS: at 19.55% examples, 490418 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-29 10:48:11,408 : INFO : EPOCH 5 - PROGRESS: at 41.69% examples, 515861 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-29 10:48:12,408 : INFO : EPOCH 5 - PROGRESS: at 63.15% examples, 521848 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:48:13,412 : INFO : EPOCH 5 - PROGRESS: at 84.78% examples, 526540 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-29 10:48:14,084 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-29 10:48:14,091 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-29 10:48:14,099 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-29 10:48:14,121 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-29 10:48:14,123 : INFO : EPOCH - 5 : training on 2988095 raw words (2494103 effective words) took 4.7s, 526875 effective words/s\n",
      "2018-05-29 10:48:14,124 : INFO : training on a 14940475 raw words (12471500 effective words) took 25.9s, 481814 effective words/s\n",
      "2018-05-29 10:48:14,216 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-05-29 10:48:14,304 : INFO : saving Word2Vec object under 300features_40minwords, separately None\n",
      "2018-05-29 10:48:14,307 : INFO : not storing attribute vectors_norm\n",
      "2018-05-29 10:48:14,309 : INFO : not storing attribute cum_table\n",
      "2018-05-29 10:48:14,769 : INFO : saved 300features_40minwords\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers = 4, size = 300, min_count = 40, window = 10, \\\n",
    "                         sample = 0.001)\n",
    "\n",
    "model.init_sims(replace = True)\n",
    "model_name = \"300features_40minwords\"\n",
    "model.save(model_name) #We can load it using Word2Vec.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'film',\n",
       " 'one',\n",
       " 'like',\n",
       " 'good',\n",
       " 'time',\n",
       " 'even',\n",
       " 'would',\n",
       " 'story',\n",
       " 'really']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "w2v_words = model.wv.index2word\n",
    "w2v_words[0:10]\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    \n",
    "#model = Word2Vec.load(\"300features_40minwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 170 of the file /Users/eve/anaconda2/envs/nlp3/lib/python3.4/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews now 5000 in 25000\n",
      "\n",
      "Reviews now 10000 in 25000\n",
      "\n",
      "Reviews now 15000 in 25000\n",
      "\n",
      "Reviews now 20000 in 25000\n",
      "\n",
      "Reviews now 25000 in 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Make function to clean data including remove html, punctuations, numbers, stopwords, lower case and split\n",
    "def cleandata(raw_data):\n",
    "    removehtml = BeautifulSoup(raw_data).get_text()\n",
    "    removenonalphabet = re.sub(\"[^a-zA-Z]\", \" \", removehtml)\n",
    "    lowersplit = removenonalphabet.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    removestop = [w for w in lowersplit if not w in stops]\n",
    "    return(\" \".join(removestop))\n",
    "\n",
    "# Get the clean_train data\n",
    "clean_train = []\n",
    "train_size = train[\"review\"].size\n",
    "for i in range(0, train_size):\n",
    "    if (i+1) % 5000 == 0:\n",
    "        print(\"Reviews now %d in %d\\n\" % (i+1, train_size))\n",
    "    clean_train.append(cleandata(train[\"review\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make vectorize & features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "feature_size = len(w2v_words)\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", stop_words = None, preprocessor = None, \\\n",
    "                             tokenizer = None, max_features = feature_size)\n",
    "\n",
    "\n",
    "train_features = vectorizer.fit_transform(clean_train)\n",
    "train_features = train_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaron',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abc',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abomination',\n",
       " 'abortion',\n",
       " 'abound']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List the features\n",
    "bag_words = vectorizer.get_feature_names()\n",
    "dist = np.sum(train_features, axis = 0)\n",
    "\n",
    "bag_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8160 8160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aaron',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abc',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abomination',\n",
       " 'abortion',\n",
       " 'abound']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_words = sorted(w2v_words, key = str.lower)\n",
    "print(len(w2v_words), len(bag_words))\n",
    "w2v_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8142"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [val for val in w2v_words if val in bag_words]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'aaron'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-71d9c0342e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'aaron'"
     ]
    }
   ],
   "source": [
    "#Apply random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rf = RandomForestClassifier(n_estimators = 100)\n",
    "model_rf = model_rf.fit(vocab, train[\"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
