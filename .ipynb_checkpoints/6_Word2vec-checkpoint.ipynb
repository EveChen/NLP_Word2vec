{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes\n",
    "#1. Word2vec can only use Python3 because Py2 have the ASC code error\n",
    "#2. Therefore, use \"source activate nlp3\" and then \"jupyter notebook\"\n",
    "\n",
    "# Word2Vec\n",
    "#1. It's a kind of method which is faster than some deep learning algorithms\n",
    "#2. Do not need labels, it can generated labels automatically based on relationships between words.\n",
    "#3. Because it cares relationships, don't remove stop words and numbers.\n",
    "#4. 關係性的比重如何算: Attemp1 Vector Average, Attemp2 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000 50000\n",
      "\n",
      "\n",
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n",
      "\n",
      "\n",
      "Index(['id', 'sentiment', 'review'], dtype='object') Index(['id', 'review'], dtype='object') Index(['id', 'review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "test = pd.read_csv(\"testData.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "\n",
    "# print\n",
    "print(train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size)\n",
    "print(\"\\n\")\n",
    "print(train[\"review\"][0])\n",
    "print(\"\\n\")\n",
    "print(train.columns, test.columns, unlabeled_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean data (remove html, non-letters, lower case, split)\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords = False):\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'all',\n",
       " 'this',\n",
       " 'stuff',\n",
       " 'going',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'with']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "list_1 = review_to_wordlist(train[\"review\"][0])\n",
    "list_1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Punkt tokenizer to split paragraphs to sentences\n",
    "# Above \"review_to_wordlist\" function is to split sentences into wordlist\n",
    "# paragraphs -> sentences -> wordlists 讓每句sentence都有自己的wordlist\n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords = False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for i in raw_sentences:\n",
    "        if len(i) > 0:\n",
    "            sentences.append(review_to_wordlist(i, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again.', 'Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent.']\n",
      "\n",
      "\n",
      "[['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again'], ['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "list_2 = tokenizer.tokenize(train[\"review\"][0].strip())\n",
    "print(list_2[0:2])\n",
    "print(\"\\n\")\n",
    "list_3 = review_to_sentences(train[\"review\"][0], tokenizer, remove_stopwords = False)\n",
    "print(list_3[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Start to make paragraph into list of words\n",
    "\n",
    "sentences = []\n",
    "for i in train[\"review\"]:\n",
    "    sentences += review_to_sentences(i, tokenizer)\n",
    "\n",
    "print(\"Parsing now\")\n",
    "for j in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(j, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "\n",
      "\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
      "\n",
      "\n",
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(\"\\n\")\n",
    "print(sentences[0])\n",
    "print(\"\\n\")\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 15:26:14,147 : INFO : collecting all words and their counts\n",
      "2018-05-26 15:26:14,149 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-26 15:26:14,256 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-26 15:26:14,354 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2018-05-26 15:26:14,450 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2018-05-26 15:26:14,549 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2018-05-26 15:26:14,678 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2018-05-26 15:26:14,755 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2018-05-26 15:26:14,840 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2018-05-26 15:26:14,929 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2018-05-26 15:26:15,011 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2018-05-26 15:26:15,089 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2018-05-26 15:26:15,199 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2018-05-26 15:26:15,354 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2018-05-26 15:26:15,480 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2018-05-26 15:26:15,588 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2018-05-26 15:26:15,718 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2018-05-26 15:26:15,829 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2018-05-26 15:26:15,945 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2018-05-26 15:26:16,065 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2018-05-26 15:26:16,177 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2018-05-26 15:26:16,290 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2018-05-26 15:26:16,399 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2018-05-26 15:26:16,522 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2018-05-26 15:26:16,640 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2018-05-26 15:26:16,758 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2018-05-26 15:26:16,870 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2018-05-26 15:26:16,984 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2018-05-26 15:26:17,127 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types\n",
      "2018-05-26 15:26:17,246 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types\n",
      "2018-05-26 15:26:17,367 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types\n",
      "2018-05-26 15:26:17,482 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types\n",
      "2018-05-26 15:26:17,593 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types\n",
      "2018-05-26 15:26:17,718 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types\n",
      "2018-05-26 15:26:17,862 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types\n",
      "2018-05-26 15:26:17,982 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types\n",
      "2018-05-26 15:26:18,104 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types\n",
      "2018-05-26 15:26:18,233 : INFO : PROGRESS: at sentence #360000, processed 8019427 words, keeping 86596 word types\n",
      "2018-05-26 15:26:18,359 : INFO : PROGRESS: at sentence #370000, processed 8246619 words, keeping 87708 word types\n",
      "2018-05-26 15:26:18,512 : INFO : PROGRESS: at sentence #380000, processed 8471766 words, keeping 88878 word types\n",
      "2018-05-26 15:26:18,636 : INFO : PROGRESS: at sentence #390000, processed 8701497 words, keeping 89907 word types\n",
      "2018-05-26 15:26:18,823 : INFO : PROGRESS: at sentence #400000, processed 8924446 words, keeping 90916 word types\n",
      "2018-05-26 15:26:18,958 : INFO : PROGRESS: at sentence #410000, processed 9145796 words, keeping 91880 word types\n",
      "2018-05-26 15:26:19,114 : INFO : PROGRESS: at sentence #420000, processed 9366876 words, keeping 92912 word types\n",
      "2018-05-26 15:26:19,236 : INFO : PROGRESS: at sentence #430000, processed 9594413 words, keeping 93932 word types\n",
      "2018-05-26 15:26:19,358 : INFO : PROGRESS: at sentence #440000, processed 9821166 words, keeping 94906 word types\n",
      "2018-05-26 15:26:19,510 : INFO : PROGRESS: at sentence #450000, processed 10044928 words, keeping 96036 word types\n",
      "2018-05-26 15:26:19,629 : INFO : PROGRESS: at sentence #460000, processed 10277688 words, keeping 97088 word types\n",
      "2018-05-26 15:26:19,769 : INFO : PROGRESS: at sentence #470000, processed 10505613 words, keeping 97933 word types\n",
      "2018-05-26 15:26:19,894 : INFO : PROGRESS: at sentence #480000, processed 10725997 words, keeping 98862 word types\n",
      "2018-05-26 15:26:20,102 : INFO : PROGRESS: at sentence #490000, processed 10952741 words, keeping 99871 word types\n",
      "2018-05-26 15:26:20,230 : INFO : PROGRESS: at sentence #500000, processed 11174397 words, keeping 100765 word types\n",
      "2018-05-26 15:26:20,363 : INFO : PROGRESS: at sentence #510000, processed 11399672 words, keeping 101699 word types\n",
      "2018-05-26 15:26:20,480 : INFO : PROGRESS: at sentence #520000, processed 11623020 words, keeping 102598 word types\n",
      "2018-05-26 15:26:20,651 : INFO : PROGRESS: at sentence #530000, processed 11847418 words, keeping 103400 word types\n",
      "2018-05-26 15:26:20,782 : INFO : PROGRESS: at sentence #540000, processed 12072033 words, keeping 104265 word types\n",
      "2018-05-26 15:26:20,908 : INFO : PROGRESS: at sentence #550000, processed 12297571 words, keeping 105133 word types\n",
      "2018-05-26 15:26:21,028 : INFO : PROGRESS: at sentence #560000, processed 12518861 words, keeping 105997 word types\n",
      "2018-05-26 15:26:21,174 : INFO : PROGRESS: at sentence #570000, processed 12747916 words, keeping 106787 word types\n",
      "2018-05-26 15:26:21,311 : INFO : PROGRESS: at sentence #580000, processed 12969412 words, keeping 107665 word types\n",
      "2018-05-26 15:26:21,449 : INFO : PROGRESS: at sentence #590000, processed 13194937 words, keeping 108501 word types\n",
      "2018-05-26 15:26:21,591 : INFO : PROGRESS: at sentence #600000, processed 13417135 words, keeping 109218 word types\n",
      "2018-05-26 15:26:21,710 : INFO : PROGRESS: at sentence #610000, processed 13638158 words, keeping 110092 word types\n",
      "2018-05-26 15:26:21,827 : INFO : PROGRESS: at sentence #620000, processed 13864483 words, keeping 110837 word types\n",
      "2018-05-26 15:26:21,932 : INFO : PROGRESS: at sentence #630000, processed 14088769 words, keeping 111610 word types\n",
      "2018-05-26 15:26:22,029 : INFO : PROGRESS: at sentence #640000, processed 14309552 words, keeping 112416 word types\n",
      "2018-05-26 15:26:22,165 : INFO : PROGRESS: at sentence #650000, processed 14535308 words, keeping 113196 word types\n",
      "2018-05-26 15:26:22,293 : INFO : PROGRESS: at sentence #660000, processed 14758098 words, keeping 113945 word types\n",
      "2018-05-26 15:26:22,414 : INFO : PROGRESS: at sentence #670000, processed 14981482 words, keeping 114643 word types\n",
      "2018-05-26 15:26:22,554 : INFO : PROGRESS: at sentence #680000, processed 15206314 words, keeping 115354 word types\n",
      "2018-05-26 15:26:22,682 : INFO : PROGRESS: at sentence #690000, processed 15428507 words, keeping 116131 word types\n",
      "2018-05-26 15:26:22,795 : INFO : PROGRESS: at sentence #700000, processed 15657213 words, keeping 116943 word types\n",
      "2018-05-26 15:26:22,904 : INFO : PROGRESS: at sentence #710000, processed 15880202 words, keeping 117596 word types\n",
      "2018-05-26 15:26:23,027 : INFO : PROGRESS: at sentence #720000, processed 16105489 words, keeping 118221 word types\n",
      "2018-05-26 15:26:23,135 : INFO : PROGRESS: at sentence #730000, processed 16331870 words, keeping 118954 word types\n",
      "2018-05-26 15:26:23,235 : INFO : PROGRESS: at sentence #740000, processed 16552903 words, keeping 119668 word types\n",
      "2018-05-26 15:26:23,342 : INFO : PROGRESS: at sentence #750000, processed 16771230 words, keeping 120295 word types\n",
      "2018-05-26 15:26:23,448 : INFO : PROGRESS: at sentence #760000, processed 16990622 words, keeping 120930 word types\n",
      "2018-05-26 15:26:23,546 : INFO : PROGRESS: at sentence #770000, processed 17217759 words, keeping 121703 word types\n",
      "2018-05-26 15:26:23,654 : INFO : PROGRESS: at sentence #780000, processed 17447905 words, keeping 122402 word types\n",
      "2018-05-26 15:26:23,774 : INFO : PROGRESS: at sentence #790000, processed 17674981 words, keeping 123066 word types\n",
      "2018-05-26 15:26:23,841 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795538 sentences\n",
      "2018-05-26 15:26:23,842 : INFO : Loading a fresh vocabulary\n",
      "2018-05-26 15:26:24,035 : INFO : min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2018-05-26 15:26:24,036 : INFO : min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)\n",
      "2018-05-26 15:26:24,109 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2018-05-26 15:26:24,125 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-05-26 15:26:24,127 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)\n",
      "2018-05-26 15:26:24,220 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2018-05-26 15:26:24,222 : INFO : resetting layer weights\n",
      "2018-05-26 15:26:24,708 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-05-26 15:26:25,742 : INFO : EPOCH 1 - PROGRESS: at 3.93% examples, 495496 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:26:26,744 : INFO : EPOCH 1 - PROGRESS: at 7.60% examples, 479314 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:26:27,763 : INFO : EPOCH 1 - PROGRESS: at 11.67% examples, 487988 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:28,771 : INFO : EPOCH 1 - PROGRESS: at 15.12% examples, 474438 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:29,781 : INFO : EPOCH 1 - PROGRESS: at 19.04% examples, 477313 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:30,783 : INFO : EPOCH 1 - PROGRESS: at 22.98% examples, 481062 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:31,790 : INFO : EPOCH 1 - PROGRESS: at 25.80% examples, 463026 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:32,808 : INFO : EPOCH 1 - PROGRESS: at 29.64% examples, 465688 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:33,809 : INFO : EPOCH 1 - PROGRESS: at 33.15% examples, 462346 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:34,829 : INFO : EPOCH 1 - PROGRESS: at 36.34% examples, 455951 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:26:35,866 : INFO : EPOCH 1 - PROGRESS: at 38.85% examples, 442393 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:26:36,870 : INFO : EPOCH 1 - PROGRESS: at 42.30% examples, 442385 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:37,873 : INFO : EPOCH 1 - PROGRESS: at 46.16% examples, 446146 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:26:38,880 : INFO : EPOCH 1 - PROGRESS: at 50.11% examples, 450285 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:26:39,894 : INFO : EPOCH 1 - PROGRESS: at 53.88% examples, 451741 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:26:40,930 : INFO : EPOCH 1 - PROGRESS: at 57.57% examples, 452380 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:41,938 : INFO : EPOCH 1 - PROGRESS: at 61.58% examples, 455805 words/s, in_qsize 7, out_qsize 2\n",
      "2018-05-26 15:26:42,946 : INFO : EPOCH 1 - PROGRESS: at 65.40% examples, 457273 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:43,955 : INFO : EPOCH 1 - PROGRESS: at 69.19% examples, 458602 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:44,970 : INFO : EPOCH 1 - PROGRESS: at 72.95% examples, 459262 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:26:45,978 : INFO : EPOCH 1 - PROGRESS: at 77.10% examples, 462369 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:26:46,993 : INFO : EPOCH 1 - PROGRESS: at 81.15% examples, 464421 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:47,994 : INFO : EPOCH 1 - PROGRESS: at 84.83% examples, 464721 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:49,000 : INFO : EPOCH 1 - PROGRESS: at 88.58% examples, 465200 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:50,030 : INFO : EPOCH 1 - PROGRESS: at 92.68% examples, 466901 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:26:51,044 : INFO : EPOCH 1 - PROGRESS: at 96.69% examples, 468202 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:51,873 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 15:26:51,875 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 15:26:51,895 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 15:26:51,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 15:26:51,909 : INFO : EPOCH - 1 : training on 17798082 raw words (12748176 effective words) took 27.2s, 469083 effective words/s\n",
      "2018-05-26 15:26:52,938 : INFO : EPOCH 2 - PROGRESS: at 3.76% examples, 474015 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:26:53,951 : INFO : EPOCH 2 - PROGRESS: at 7.42% examples, 466267 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:54,964 : INFO : EPOCH 2 - PROGRESS: at 11.67% examples, 487646 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:56,002 : INFO : EPOCH 2 - PROGRESS: at 14.56% examples, 453084 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:57,006 : INFO : EPOCH 2 - PROGRESS: at 17.96% examples, 447956 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:26:58,017 : INFO : EPOCH 2 - PROGRESS: at 22.15% examples, 460400 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:26:59,018 : INFO : EPOCH 2 - PROGRESS: at 25.69% examples, 459031 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:00,029 : INFO : EPOCH 2 - PROGRESS: at 29.58% examples, 463488 words/s, in_qsize 7, out_qsize 1\n",
      "2018-05-26 15:27:01,048 : INFO : EPOCH 2 - PROGRESS: at 33.83% examples, 469602 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:27:02,053 : INFO : EPOCH 2 - PROGRESS: at 37.84% examples, 473836 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:27:03,080 : INFO : EPOCH 2 - PROGRESS: at 42.02% examples, 478305 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:04,085 : INFO : EPOCH 2 - PROGRESS: at 46.22% examples, 482841 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:05,088 : INFO : EPOCH 2 - PROGRESS: at 50.28% examples, 485738 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:06,095 : INFO : EPOCH 2 - PROGRESS: at 54.31% examples, 487560 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:07,098 : INFO : EPOCH 2 - PROGRESS: at 58.35% examples, 489718 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:08,108 : INFO : EPOCH 2 - PROGRESS: at 62.49% examples, 491839 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:09,116 : INFO : EPOCH 2 - PROGRESS: at 66.57% examples, 493408 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:10,119 : INFO : EPOCH 2 - PROGRESS: at 70.60% examples, 494525 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:11,124 : INFO : EPOCH 2 - PROGRESS: at 74.63% examples, 495472 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:27:12,130 : INFO : EPOCH 2 - PROGRESS: at 78.79% examples, 496969 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:13,138 : INFO : EPOCH 2 - PROGRESS: at 82.89% examples, 497945 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:14,144 : INFO : EPOCH 2 - PROGRESS: at 86.97% examples, 498884 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:15,147 : INFO : EPOCH 2 - PROGRESS: at 90.98% examples, 499534 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:16,155 : INFO : EPOCH 2 - PROGRESS: at 95.11% examples, 500291 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:17,162 : INFO : EPOCH 2 - PROGRESS: at 99.22% examples, 501311 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:17,322 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 15:27:17,328 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 15:27:17,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 15:27:17,352 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 15:27:17,354 : INFO : EPOCH - 2 : training on 17798082 raw words (12750323 effective words) took 25.4s, 501444 effective words/s\n",
      "2018-05-26 15:27:18,378 : INFO : EPOCH 3 - PROGRESS: at 3.81% examples, 483259 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:19,383 : INFO : EPOCH 3 - PROGRESS: at 7.77% examples, 490295 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:20,395 : INFO : EPOCH 3 - PROGRESS: at 11.67% examples, 489556 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:21,403 : INFO : EPOCH 3 - PROGRESS: at 15.45% examples, 486273 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:22,414 : INFO : EPOCH 3 - PROGRESS: at 19.20% examples, 482387 words/s, in_qsize 8, out_qsize 1\n",
      "2018-05-26 15:27:23,417 : INFO : EPOCH 3 - PROGRESS: at 23.15% examples, 485189 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:24,424 : INFO : EPOCH 3 - PROGRESS: at 27.14% examples, 487850 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:25,436 : INFO : EPOCH 3 - PROGRESS: at 30.92% examples, 485956 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:26,451 : INFO : EPOCH 3 - PROGRESS: at 34.60% examples, 482716 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:27,453 : INFO : EPOCH 3 - PROGRESS: at 38.40% examples, 482986 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:28,481 : INFO : EPOCH 3 - PROGRESS: at 42.41% examples, 484715 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:29,487 : INFO : EPOCH 3 - PROGRESS: at 46.34% examples, 485750 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:30,488 : INFO : EPOCH 3 - PROGRESS: at 50.16% examples, 486299 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:31,497 : INFO : EPOCH 3 - PROGRESS: at 53.99% examples, 485971 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:32,498 : INFO : EPOCH 3 - PROGRESS: at 57.73% examples, 485895 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:33,511 : INFO : EPOCH 3 - PROGRESS: at 61.64% examples, 486401 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:34,520 : INFO : EPOCH 3 - PROGRESS: at 65.68% examples, 487823 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:35,527 : INFO : EPOCH 3 - PROGRESS: at 69.65% examples, 488748 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:27:36,532 : INFO : EPOCH 3 - PROGRESS: at 73.56% examples, 489255 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:27:37,535 : INFO : EPOCH 3 - PROGRESS: at 77.43% examples, 489384 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:38,553 : INFO : EPOCH 3 - PROGRESS: at 81.26% examples, 488818 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:39,559 : INFO : EPOCH 3 - PROGRESS: at 85.23% examples, 489560 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:40,562 : INFO : EPOCH 3 - PROGRESS: at 89.21% examples, 490253 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:41,564 : INFO : EPOCH 3 - PROGRESS: at 93.12% examples, 490638 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:42,587 : INFO : EPOCH 3 - PROGRESS: at 97.18% examples, 491152 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:43,283 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 15:27:43,297 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 15:27:43,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 15:27:43,313 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 15:27:43,314 : INFO : EPOCH - 3 : training on 17798082 raw words (12748966 effective words) took 25.9s, 491410 effective words/s\n",
      "2018-05-26 15:27:44,324 : INFO : EPOCH 4 - PROGRESS: at 3.76% examples, 479065 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:45,339 : INFO : EPOCH 4 - PROGRESS: at 7.66% examples, 482029 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:46,347 : INFO : EPOCH 4 - PROGRESS: at 11.44% examples, 479716 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:47,375 : INFO : EPOCH 4 - PROGRESS: at 15.40% examples, 481808 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:48,375 : INFO : EPOCH 4 - PROGRESS: at 19.26% examples, 482643 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:49,383 : INFO : EPOCH 4 - PROGRESS: at 23.15% examples, 483815 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:50,398 : INFO : EPOCH 4 - PROGRESS: at 27.25% examples, 488159 words/s, in_qsize 7, out_qsize 1\n",
      "2018-05-26 15:27:51,417 : INFO : EPOCH 4 - PROGRESS: at 31.43% examples, 491972 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:52,421 : INFO : EPOCH 4 - PROGRESS: at 35.39% examples, 492560 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:27:53,425 : INFO : EPOCH 4 - PROGRESS: at 39.31% examples, 493227 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:54,440 : INFO : EPOCH 4 - PROGRESS: at 43.30% examples, 494510 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:55,441 : INFO : EPOCH 4 - PROGRESS: at 47.17% examples, 494377 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:56,453 : INFO : EPOCH 4 - PROGRESS: at 51.13% examples, 494971 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:57,457 : INFO : EPOCH 4 - PROGRESS: at 55.05% examples, 495222 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:27:58,467 : INFO : EPOCH 4 - PROGRESS: at 59.00% examples, 496142 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:27:59,489 : INFO : EPOCH 4 - PROGRESS: at 62.94% examples, 495742 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:00,531 : INFO : EPOCH 4 - PROGRESS: at 66.69% examples, 493562 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:28:01,533 : INFO : EPOCH 4 - PROGRESS: at 69.99% examples, 489568 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:02,548 : INFO : EPOCH 4 - PROGRESS: at 73.62% examples, 487927 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:28:03,570 : INFO : EPOCH 4 - PROGRESS: at 77.60% examples, 488358 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:28:04,583 : INFO : EPOCH 4 - PROGRESS: at 81.21% examples, 486623 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:28:05,586 : INFO : EPOCH 4 - PROGRESS: at 85.23% examples, 487824 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:06,593 : INFO : EPOCH 4 - PROGRESS: at 89.10% examples, 487920 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:07,607 : INFO : EPOCH 4 - PROGRESS: at 93.01% examples, 488160 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:08,622 : INFO : EPOCH 4 - PROGRESS: at 97.02% examples, 488637 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:28:09,352 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 15:28:09,361 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 15:28:09,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 15:28:09,382 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 15:28:09,383 : INFO : EPOCH - 4 : training on 17798082 raw words (12748370 effective words) took 26.1s, 489196 effective words/s\n",
      "2018-05-26 15:28:10,397 : INFO : EPOCH 5 - PROGRESS: at 3.99% examples, 504993 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:11,419 : INFO : EPOCH 5 - PROGRESS: at 8.11% examples, 507684 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:12,421 : INFO : EPOCH 5 - PROGRESS: at 11.95% examples, 500539 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:13,438 : INFO : EPOCH 5 - PROGRESS: at 15.73% examples, 493235 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:14,445 : INFO : EPOCH 5 - PROGRESS: at 18.75% examples, 469817 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:15,460 : INFO : EPOCH 5 - PROGRESS: at 22.03% examples, 459553 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:28:16,471 : INFO : EPOCH 5 - PROGRESS: at 25.05% examples, 448459 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:17,495 : INFO : EPOCH 5 - PROGRESS: at 28.31% examples, 442931 words/s, in_qsize 8, out_qsize 1\n",
      "2018-05-26 15:28:18,495 : INFO : EPOCH 5 - PROGRESS: at 31.67% examples, 440596 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:19,510 : INFO : EPOCH 5 - PROGRESS: at 34.70% examples, 434526 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:20,516 : INFO : EPOCH 5 - PROGRESS: at 38.40% examples, 437607 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:21,516 : INFO : EPOCH 5 - PROGRESS: at 42.35% examples, 443414 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:22,536 : INFO : EPOCH 5 - PROGRESS: at 46.40% examples, 448163 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-26 15:28:23,538 : INFO : EPOCH 5 - PROGRESS: at 49.94% examples, 448765 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:24,538 : INFO : EPOCH 5 - PROGRESS: at 52.94% examples, 444144 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:25,543 : INFO : EPOCH 5 - PROGRESS: at 56.27% examples, 443075 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:26,548 : INFO : EPOCH 5 - PROGRESS: at 60.06% examples, 445876 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:27,551 : INFO : EPOCH 5 - PROGRESS: at 64.11% examples, 449605 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:28:28,555 : INFO : EPOCH 5 - PROGRESS: at 68.10% examples, 452574 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-26 15:28:29,573 : INFO : EPOCH 5 - PROGRESS: at 72.15% examples, 455602 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:30,573 : INFO : EPOCH 5 - PROGRESS: at 76.16% examples, 458072 words/s, in_qsize 8, out_qsize 1\n",
      "2018-05-26 15:28:31,708 : INFO : EPOCH 5 - PROGRESS: at 79.98% examples, 456566 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:32,708 : INFO : EPOCH 5 - PROGRESS: at 83.39% examples, 455700 words/s, in_qsize 7, out_qsize 1\n",
      "2018-05-26 15:28:33,720 : INFO : EPOCH 5 - PROGRESS: at 87.02% examples, 455869 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:34,727 : INFO : EPOCH 5 - PROGRESS: at 90.71% examples, 456357 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:35,745 : INFO : EPOCH 5 - PROGRESS: at 94.32% examples, 456120 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:36,754 : INFO : EPOCH 5 - PROGRESS: at 98.11% examples, 457088 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-26 15:28:37,180 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-26 15:28:37,198 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-26 15:28:37,211 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-26 15:28:37,218 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-26 15:28:37,219 : INFO : EPOCH - 5 : training on 17798082 raw words (12749559 effective words) took 27.8s, 458173 effective words/s\n",
      "2018-05-26 15:28:37,221 : INFO : training on a 88990410 raw words (63745394 effective words) took 132.5s, 481058 effective words/s\n",
      "2018-05-26 15:28:37,224 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-05-26 15:28:37,387 : INFO : saving Word2Vec object under 300features_40minwords, separately None\n",
      "2018-05-26 15:28:37,389 : INFO : not storing attribute vectors_norm\n",
      "2018-05-26 15:28:37,395 : INFO : not storing attribute cum_table\n",
      "2018-05-26 15:28:38,202 : INFO : saved 300features_40minwords\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers = 4, size = 300, min_count = 40, window = 10, \\\n",
    "                         sample = 0.001)\n",
    "\n",
    "model.init_sims(replace = True)\n",
    "model_name = \"300features_40minwords\"\n",
    "model.save(model_name) #We can load it using Word2Vec.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'monkey'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"guy girl woman man monkey\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('turtle', 0.6487868428230286),\n",
       " ('chicken', 0.6476317048072815),\n",
       " ('gorilla', 0.6274864673614502),\n",
       " ('rabbit', 0.6065736413002014),\n",
       " ('giant', 0.6062736511230469),\n",
       " ('sock', 0.6038341522216797),\n",
       " ('bowl', 0.5888091325759888),\n",
       " ('dinosaur', 0.5883542895317078),\n",
       " ('midget', 0.5821043252944946),\n",
       " ('pig', 0.5818576216697693)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"monkey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('boy', 0.7197983860969543),\n",
       " ('woman', 0.6685178875923157),\n",
       " ('prostitute', 0.6315896511077881),\n",
       " ('teenager', 0.5830933451652527),\n",
       " ('girls', 0.5804745554924011),\n",
       " ('gal', 0.5762826800346375),\n",
       " ('nun', 0.56174236536026),\n",
       " ('lady', 0.559321403503418),\n",
       " ('lad', 0.5577070713043213),\n",
       " ('daughter', 0.5506519675254822)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'it',\n",
       " 'in',\n",
       " 'i',\n",
       " 'this',\n",
       " 'that',\n",
       " 's',\n",
       " 'was',\n",
       " 'as',\n",
       " 'with',\n",
       " 'for',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'film',\n",
       " 'you',\n",
       " 't',\n",
       " 'on',\n",
       " 'not',\n",
       " 'he',\n",
       " 'are',\n",
       " 'his',\n",
       " 'have',\n",
       " 'be',\n",
       " 'one',\n",
       " 'all',\n",
       " 'at',\n",
       " 'they',\n",
       " 'by',\n",
       " 'who',\n",
       " 'an',\n",
       " 'from',\n",
       " 'so',\n",
       " 'like',\n",
       " 'there',\n",
       " 'her',\n",
       " 'or',\n",
       " 'just',\n",
       " 'about',\n",
       " 'out',\n",
       " 'has',\n",
       " 'if',\n",
       " 'what',\n",
       " 'some',\n",
       " 'good',\n",
       " 'can',\n",
       " 'more',\n",
       " 'when',\n",
       " 'very',\n",
       " 'she',\n",
       " 'up',\n",
       " 'no',\n",
       " 'time',\n",
       " 'even',\n",
       " 'would',\n",
       " 'my',\n",
       " 'which',\n",
       " 'their',\n",
       " 'story',\n",
       " 'only',\n",
       " 'really',\n",
       " 'see',\n",
       " 'had',\n",
       " 'were',\n",
       " 'well',\n",
       " 'we',\n",
       " 'me',\n",
       " 'than',\n",
       " 'much',\n",
       " 'bad',\n",
       " 'get',\n",
       " 'been',\n",
       " 'people',\n",
       " 'also',\n",
       " 'into',\n",
       " 'do',\n",
       " 'great',\n",
       " 'other',\n",
       " 'will',\n",
       " 'first',\n",
       " 'because',\n",
       " 'him',\n",
       " 'how',\n",
       " 'most',\n",
       " 'don',\n",
       " 'them',\n",
       " 'made',\n",
       " 'its',\n",
       " 'make',\n",
       " 'then',\n",
       " 'way',\n",
       " 'could',\n",
       " 'too',\n",
       " 'movies',\n",
       " 'after',\n",
       " 'any',\n",
       " 'characters',\n",
       " 'character',\n",
       " 'think',\n",
       " 'films',\n",
       " 'two',\n",
       " 'watch',\n",
       " 'being',\n",
       " 'many',\n",
       " 'plot',\n",
       " 'seen',\n",
       " 'never',\n",
       " 'where',\n",
       " 'love',\n",
       " 'life',\n",
       " 'little',\n",
       " 'acting',\n",
       " 'best',\n",
       " 'did',\n",
       " 'over',\n",
       " 'off',\n",
       " 'know',\n",
       " 'show',\n",
       " 'ever',\n",
       " 'does',\n",
       " 'man',\n",
       " 'better',\n",
       " 'your',\n",
       " 'here',\n",
       " 'end',\n",
       " 'scene',\n",
       " 'these',\n",
       " 'still',\n",
       " 'while',\n",
       " 'why',\n",
       " 'scenes',\n",
       " 'say',\n",
       " 'something',\n",
       " 'go',\n",
       " 've',\n",
       " 'm',\n",
       " 'should',\n",
       " 'such',\n",
       " 'back',\n",
       " 'through',\n",
       " 'real',\n",
       " 'those',\n",
       " 'now',\n",
       " 're',\n",
       " 'watching',\n",
       " 'doesn',\n",
       " 'thing',\n",
       " 'though',\n",
       " 'director',\n",
       " 'years',\n",
       " 'actors',\n",
       " 'funny',\n",
       " 'old',\n",
       " 'didn',\n",
       " 'another',\n",
       " 'new',\n",
       " 'nothing',\n",
       " 'makes',\n",
       " 'work',\n",
       " 'actually',\n",
       " 'going',\n",
       " 'before',\n",
       " 'look',\n",
       " 'find',\n",
       " 'same',\n",
       " 'lot',\n",
       " 'few',\n",
       " 'part',\n",
       " 'every',\n",
       " 'again',\n",
       " 'world',\n",
       " 'cast',\n",
       " 'us',\n",
       " 'things',\n",
       " 'horror',\n",
       " 'quite',\n",
       " 'want',\n",
       " 'action',\n",
       " 'down',\n",
       " 'pretty',\n",
       " 'young',\n",
       " 'around',\n",
       " 'seems',\n",
       " 'fact',\n",
       " 'take',\n",
       " 'however',\n",
       " 'enough',\n",
       " 'got',\n",
       " 'long',\n",
       " 'both',\n",
       " 'thought',\n",
       " 'big',\n",
       " 'own',\n",
       " 'give',\n",
       " 'between',\n",
       " 'd',\n",
       " 'comedy',\n",
       " 'series',\n",
       " 'must',\n",
       " 'may',\n",
       " 'right',\n",
       " 'original',\n",
       " 'without',\n",
       " 'role',\n",
       " 'interesting',\n",
       " 'come',\n",
       " 'times',\n",
       " 'always',\n",
       " 'isn',\n",
       " 'guy',\n",
       " 'saw',\n",
       " 'whole',\n",
       " 'gets',\n",
       " 'least',\n",
       " 'point',\n",
       " 'almost',\n",
       " 'bit',\n",
       " 'script',\n",
       " 'music',\n",
       " 'done',\n",
       " 'last',\n",
       " 'minutes',\n",
       " 'far',\n",
       " 'family',\n",
       " 'feel',\n",
       " 'since',\n",
       " 'making',\n",
       " 'll',\n",
       " 'girl',\n",
       " 'might',\n",
       " 'performance',\n",
       " 'anything',\n",
       " 'yet',\n",
       " 'away',\n",
       " 'probably',\n",
       " 'am',\n",
       " 'woman',\n",
       " 'tv',\n",
       " 'kind',\n",
       " 'hard',\n",
       " 'fun',\n",
       " 'rather',\n",
       " 'day',\n",
       " 'worst',\n",
       " 'sure',\n",
       " 'played',\n",
       " 'anyone',\n",
       " 'each',\n",
       " 'found',\n",
       " 'especially',\n",
       " 'having',\n",
       " 'our',\n",
       " 'trying',\n",
       " 'screen',\n",
       " 'looking',\n",
       " 'believe',\n",
       " 'different',\n",
       " 'although',\n",
       " 'place',\n",
       " 'course',\n",
       " 'goes',\n",
       " 'sense',\n",
       " 'set',\n",
       " 'comes',\n",
       " 'ending',\n",
       " 'maybe',\n",
       " 'shows',\n",
       " 'worth',\n",
       " 'american',\n",
       " 'three',\n",
       " 'dvd',\n",
       " 'money',\n",
       " 'put',\n",
       " 'looks',\n",
       " 'once',\n",
       " 'everything',\n",
       " 'actor',\n",
       " 'someone',\n",
       " 'let',\n",
       " 'plays',\n",
       " 'effects',\n",
       " 'main',\n",
       " 'john',\n",
       " 'wasn',\n",
       " 'year',\n",
       " 'together',\n",
       " 'reason',\n",
       " 'book',\n",
       " 'true',\n",
       " 'everyone',\n",
       " 'during',\n",
       " 'instead',\n",
       " 'takes',\n",
       " 'said',\n",
       " 'job',\n",
       " 'high',\n",
       " 'play',\n",
       " 'special',\n",
       " 'war',\n",
       " 'seem',\n",
       " 'night',\n",
       " 'watched',\n",
       " 'later',\n",
       " 'audience',\n",
       " 'wife',\n",
       " 'himself',\n",
       " 'star',\n",
       " 'black',\n",
       " 'half',\n",
       " 'seeing',\n",
       " 'left',\n",
       " 'death',\n",
       " 'idea',\n",
       " 'excellent',\n",
       " 'beautiful',\n",
       " 'shot',\n",
       " 'house',\n",
       " 'second',\n",
       " 'father',\n",
       " 'simply',\n",
       " 'used',\n",
       " 'men',\n",
       " 'dead',\n",
       " 'else',\n",
       " 'mind',\n",
       " 'version',\n",
       " 'less',\n",
       " 'completely',\n",
       " 'hollywood',\n",
       " 'nice',\n",
       " 'poor',\n",
       " 'fan',\n",
       " 'budget',\n",
       " 'women',\n",
       " 'help',\n",
       " 'home',\n",
       " 'line',\n",
       " 'sex',\n",
       " 'boring',\n",
       " 'performances',\n",
       " 'along',\n",
       " 'try',\n",
       " 'top',\n",
       " 'either',\n",
       " 'short',\n",
       " 'read',\n",
       " 'low',\n",
       " 'wrong',\n",
       " 'use',\n",
       " 'until',\n",
       " 'friends',\n",
       " 'camera',\n",
       " 'kids',\n",
       " 'given',\n",
       " 'couple',\n",
       " 'next',\n",
       " 'need',\n",
       " 'start',\n",
       " 'enjoy',\n",
       " 'full',\n",
       " 'classic',\n",
       " 'production',\n",
       " 'rest',\n",
       " 'truly',\n",
       " 'perhaps',\n",
       " 'stupid',\n",
       " 'awful',\n",
       " 'school',\n",
       " 'moments',\n",
       " 'video',\n",
       " 'mother',\n",
       " 'tell',\n",
       " 'mean',\n",
       " 'getting',\n",
       " 'keep',\n",
       " 'face',\n",
       " 'came',\n",
       " 'won',\n",
       " 'understand',\n",
       " 'small',\n",
       " 'terrible',\n",
       " 'others',\n",
       " 'recommend',\n",
       " 'name',\n",
       " 'style',\n",
       " 'playing',\n",
       " 'itself',\n",
       " 'boy',\n",
       " 'wonderful',\n",
       " 'doing',\n",
       " 'stars',\n",
       " 'remember',\n",
       " 'person',\n",
       " 'definitely',\n",
       " 'gives',\n",
       " 'often',\n",
       " 'lost',\n",
       " 'dialogue',\n",
       " 'written',\n",
       " 'live',\n",
       " 'early',\n",
       " 'lines',\n",
       " 'perfect',\n",
       " 'human',\n",
       " 'case',\n",
       " 'entertaining',\n",
       " 'head',\n",
       " 'yes',\n",
       " 'title',\n",
       " 'become',\n",
       " 'went',\n",
       " 'couldn',\n",
       " 'hope',\n",
       " 'episode',\n",
       " 'children',\n",
       " 'liked',\n",
       " 'friend',\n",
       " 'certainly',\n",
       " 'based',\n",
       " 'supposed',\n",
       " 'picture',\n",
       " 'piece',\n",
       " 'problem',\n",
       " 'finally',\n",
       " 'against',\n",
       " 'absolutely',\n",
       " 'oh',\n",
       " 'drama',\n",
       " 'fans',\n",
       " 'sort',\n",
       " 'several',\n",
       " 'overall',\n",
       " 'cinema',\n",
       " 'entire',\n",
       " 'felt',\n",
       " 'under',\n",
       " 'son',\n",
       " 'worse',\n",
       " 'laugh',\n",
       " 'called',\n",
       " 'evil',\n",
       " 'direction',\n",
       " 'lives',\n",
       " 'waste',\n",
       " 'killer',\n",
       " 'lead',\n",
       " 'humor',\n",
       " 'guys',\n",
       " 'care',\n",
       " 'beginning',\n",
       " 'white',\n",
       " 'dark',\n",
       " 'game',\n",
       " 'despite',\n",
       " 'seemed',\n",
       " 'final',\n",
       " 'b',\n",
       " 'becomes',\n",
       " 'wanted',\n",
       " 'unfortunately',\n",
       " 'throughout',\n",
       " 'mr',\n",
       " 'loved',\n",
       " 'totally',\n",
       " 'history',\n",
       " 'already',\n",
       " 'genre',\n",
       " 'turn',\n",
       " 'town',\n",
       " 'guess',\n",
       " 'fine',\n",
       " 'able',\n",
       " 'days',\n",
       " 'heart',\n",
       " 'city',\n",
       " 'flick',\n",
       " 'act',\n",
       " 'run',\n",
       " 'side',\n",
       " 'wants',\n",
       " 'quality',\n",
       " 'today',\n",
       " 'tries',\n",
       " 'child',\n",
       " 'hand',\n",
       " 'sound',\n",
       " 'kill',\n",
       " 'close',\n",
       " 'horrible',\n",
       " 'past',\n",
       " 'example',\n",
       " 'starts',\n",
       " 'writing',\n",
       " 'viewer',\n",
       " 'turns',\n",
       " 'themselves',\n",
       " 'amazing',\n",
       " 'enjoyed',\n",
       " 'etc',\n",
       " 'car',\n",
       " 'parts',\n",
       " 'behind',\n",
       " 'directed',\n",
       " 'works',\n",
       " 'expect',\n",
       " 'michael',\n",
       " 'killed',\n",
       " 'matter',\n",
       " 'daughter',\n",
       " 'soon',\n",
       " 'fight',\n",
       " 'favorite',\n",
       " 'kid',\n",
       " 'self',\n",
       " 'decent',\n",
       " 'stuff',\n",
       " 'gave',\n",
       " 'blood',\n",
       " 'sometimes',\n",
       " 'type',\n",
       " 'actress',\n",
       " 'eyes',\n",
       " 'thinking',\n",
       " 'group',\n",
       " 'girls',\n",
       " 'art',\n",
       " 'violence',\n",
       " 'obviously',\n",
       " 'brilliant',\n",
       " 'stop',\n",
       " 'stories',\n",
       " 'late',\n",
       " 'hour',\n",
       " 'known',\n",
       " 'myself',\n",
       " 'except',\n",
       " 'writer',\n",
       " 'happened',\n",
       " 'hero',\n",
       " 'god',\n",
       " 'says',\n",
       " 'feeling',\n",
       " 'highly',\n",
       " 'coming',\n",
       " 'heard',\n",
       " 'roles',\n",
       " 'police',\n",
       " 'extremely',\n",
       " 'took',\n",
       " 'happens',\n",
       " 'slow',\n",
       " 'leave',\n",
       " 'experience',\n",
       " 'moment',\n",
       " 'husband',\n",
       " 'anyway',\n",
       " 'voice',\n",
       " 'hell',\n",
       " 'wouldn',\n",
       " 'murder',\n",
       " 'attempt',\n",
       " 'involved',\n",
       " 'age',\n",
       " 'obvious',\n",
       " 'living',\n",
       " 'interest',\n",
       " 'including',\n",
       " 'score',\n",
       " 'strong',\n",
       " 'looked',\n",
       " 'taken',\n",
       " 'told',\n",
       " 'david',\n",
       " 'save',\n",
       " 'brother',\n",
       " 'ok',\n",
       " 'wonder',\n",
       " 'none',\n",
       " 'happen',\n",
       " 'cut',\n",
       " 'hours',\n",
       " 'career',\n",
       " 'please',\n",
       " 'cool',\n",
       " 'robert',\n",
       " 'chance',\n",
       " 'particularly',\n",
       " 'gore',\n",
       " 'james',\n",
       " 'cannot',\n",
       " 'simple',\n",
       " 'hit',\n",
       " 'across',\n",
       " 'ago',\n",
       " 'complete',\n",
       " 'exactly',\n",
       " 'lack',\n",
       " 'hilarious',\n",
       " 'crap',\n",
       " 'o',\n",
       " 'annoying',\n",
       " 'possible',\n",
       " 'alone',\n",
       " 'power',\n",
       " 'relationship',\n",
       " 'light',\n",
       " 'sad',\n",
       " 'serious',\n",
       " 'level',\n",
       " 'important',\n",
       " 'running',\n",
       " 'documentary',\n",
       " 'seriously',\n",
       " 'whose',\n",
       " 'usually',\n",
       " 'female',\n",
       " 'reality',\n",
       " 'ends',\n",
       " 'scary',\n",
       " 'order',\n",
       " 'somewhat',\n",
       " 'talent',\n",
       " 'happy',\n",
       " 'finds',\n",
       " 'taking',\n",
       " 'song',\n",
       " 'number',\n",
       " 'middle',\n",
       " 'shown',\n",
       " 'room',\n",
       " 'ridiculous',\n",
       " 'strange',\n",
       " 'change',\n",
       " 'call',\n",
       " 'basically',\n",
       " 'released',\n",
       " 'usual',\n",
       " 'body',\n",
       " 'opening',\n",
       " 'jokes',\n",
       " 'turned',\n",
       " 'mostly',\n",
       " 'english',\n",
       " 'country',\n",
       " 'wish',\n",
       " 'apparently',\n",
       " 'yourself',\n",
       " 'cinematography',\n",
       " 'opinion',\n",
       " 'silly',\n",
       " 'novel',\n",
       " 'attention',\n",
       " 'view',\n",
       " 'four',\n",
       " 'started',\n",
       " 'word',\n",
       " 'saying',\n",
       " 'jack',\n",
       " 'disappointed',\n",
       " 'sequel',\n",
       " 'miss',\n",
       " 'single',\n",
       " 'talking',\n",
       " 'huge',\n",
       " 'thriller',\n",
       " 'future',\n",
       " 'clich',\n",
       " 'shots',\n",
       " 'words',\n",
       " 'major',\n",
       " 'cheap',\n",
       " 'straight',\n",
       " 'non',\n",
       " 'clearly',\n",
       " 'rating',\n",
       " 'modern',\n",
       " 'beyond',\n",
       " 'knows',\n",
       " 'knew',\n",
       " 'ones',\n",
       " 'due',\n",
       " 'fast',\n",
       " 'problems',\n",
       " 'events',\n",
       " 'british',\n",
       " 'sets',\n",
       " 'king',\n",
       " 'talk',\n",
       " 'tells',\n",
       " 'comic',\n",
       " 'french',\n",
       " 'parents',\n",
       " 'bring',\n",
       " 'die',\n",
       " 'easily',\n",
       " 'aren',\n",
       " 'entertainment',\n",
       " 'local',\n",
       " 'earth',\n",
       " 'add',\n",
       " 'class',\n",
       " 'sequence',\n",
       " 'upon',\n",
       " 'george',\n",
       " 'above',\n",
       " 'musical',\n",
       " 'television',\n",
       " 'within',\n",
       " 'giving',\n",
       " 'falls',\n",
       " 'similar',\n",
       " 'storyline',\n",
       " 'york',\n",
       " 'supporting',\n",
       " 'ten',\n",
       " 'clear',\n",
       " 'mystery',\n",
       " 'haven',\n",
       " 'easy',\n",
       " 'appears',\n",
       " 'romantic',\n",
       " 'hate',\n",
       " 'five',\n",
       " 'predictable',\n",
       " 'review',\n",
       " 'near',\n",
       " 'typical',\n",
       " 'lots',\n",
       " 'ways',\n",
       " 'bunch',\n",
       " 'team',\n",
       " 'enjoyable',\n",
       " 'begins',\n",
       " 'named',\n",
       " 'dialog',\n",
       " 'general',\n",
       " 'stand',\n",
       " 'crime',\n",
       " 'working',\n",
       " 'elements',\n",
       " 'mention',\n",
       " 'eye',\n",
       " 'message',\n",
       " 'e',\n",
       " 'theme',\n",
       " 'filmed',\n",
       " 'richard',\n",
       " 'episodes',\n",
       " 'points',\n",
       " 'certain',\n",
       " 'avoid',\n",
       " 'songs',\n",
       " 'red',\n",
       " 'america',\n",
       " 'tale',\n",
       " 'sorry',\n",
       " 'whether',\n",
       " 'release',\n",
       " 'gay',\n",
       " 'dull',\n",
       " 'surprised',\n",
       " 'moving',\n",
       " 'among',\n",
       " 'tom',\n",
       " 'th',\n",
       " 'viewers',\n",
       " 'stay',\n",
       " 'de',\n",
       " 'using',\n",
       " 'minute',\n",
       " 'fall',\n",
       " 'needs',\n",
       " 'effort',\n",
       " 'feels',\n",
       " 'gone',\n",
       " 'space',\n",
       " 'lee',\n",
       " 'leads',\n",
       " 'kept',\n",
       " 'paul',\n",
       " 'nearly',\n",
       " 'theater',\n",
       " 'tried',\n",
       " 'herself',\n",
       " 'comments',\n",
       " 'means',\n",
       " 'peter',\n",
       " 'period',\n",
       " 'showing',\n",
       " 'third',\n",
       " 'truth',\n",
       " 'sister',\n",
       " 'brought',\n",
       " 'suspense',\n",
       " 'doubt',\n",
       " 'buy',\n",
       " 'soundtrack',\n",
       " 'somehow',\n",
       " 'lady',\n",
       " 'killing',\n",
       " 'feature',\n",
       " 'follow',\n",
       " 'sequences',\n",
       " 'viewing',\n",
       " 'fantastic',\n",
       " 'editing',\n",
       " 'form',\n",
       " 'famous',\n",
       " 'material',\n",
       " 'realistic',\n",
       " 'rent',\n",
       " 'average',\n",
       " 'cop',\n",
       " 'okay',\n",
       " 'dog',\n",
       " 'check',\n",
       " 'whatever',\n",
       " 'monster',\n",
       " 'rock',\n",
       " 'reviews',\n",
       " 'imagine',\n",
       " 'move',\n",
       " 'figure',\n",
       " 'oscar',\n",
       " 'surprise',\n",
       " 'forget',\n",
       " 'lame',\n",
       " 'fi',\n",
       " 'premise',\n",
       " 'believable',\n",
       " 'weak',\n",
       " 'animation',\n",
       " 'indeed',\n",
       " 'deal',\n",
       " 'poorly',\n",
       " 'sci',\n",
       " 'free',\n",
       " 'possibly',\n",
       " 'actual',\n",
       " 'expected',\n",
       " 'learn',\n",
       " 'hear',\n",
       " 'eventually',\n",
       " 'dr',\n",
       " 'stage',\n",
       " 'forced',\n",
       " 'sexual',\n",
       " 'atmosphere',\n",
       " 'note',\n",
       " 'deep',\n",
       " 'society',\n",
       " 'greatest',\n",
       " 'sit',\n",
       " 'otherwise',\n",
       " 'open',\n",
       " 'wait',\n",
       " 'leaves',\n",
       " 'difficult',\n",
       " 'question',\n",
       " 'romance',\n",
       " 'decided',\n",
       " 'screenplay',\n",
       " 'begin',\n",
       " 'reading',\n",
       " 'plus',\n",
       " 'joe',\n",
       " 'situation',\n",
       " 'western',\n",
       " 'became',\n",
       " 'particular',\n",
       " 'subject',\n",
       " 'earlier',\n",
       " 'hot',\n",
       " 'nor',\n",
       " 'male',\n",
       " 'towards',\n",
       " 'box',\n",
       " 'crew',\n",
       " 'gun',\n",
       " 'brothers',\n",
       " 'interested',\n",
       " 'personal',\n",
       " 'acted',\n",
       " 'street',\n",
       " 'meet',\n",
       " 'credits',\n",
       " 'previous',\n",
       " 'cheesy',\n",
       " 'imdb',\n",
       " 'footage',\n",
       " 'business',\n",
       " 'powerful',\n",
       " 'memorable',\n",
       " 'worked',\n",
       " 'battle',\n",
       " 'shame',\n",
       " 'writers',\n",
       " 'mess',\n",
       " 'effect',\n",
       " 'laughs',\n",
       " 'season',\n",
       " 'features',\n",
       " 'whom',\n",
       " 'result',\n",
       " 'dramatic',\n",
       " 'older',\n",
       " 'air',\n",
       " 'setting',\n",
       " 'perfectly',\n",
       " 'unless',\n",
       " 'quickly',\n",
       " 'era',\n",
       " 'needed',\n",
       " 'keeps',\n",
       " 'nature',\n",
       " 'boys',\n",
       " 'hands',\n",
       " 'baby',\n",
       " 'bill',\n",
       " 'crazy',\n",
       " 'badly',\n",
       " 'total',\n",
       " 'background',\n",
       " 'directing',\n",
       " 'realize',\n",
       " 'emotional',\n",
       " 'mark',\n",
       " 'forward',\n",
       " 'comment',\n",
       " 'present',\n",
       " 'japanese',\n",
       " 'appear',\n",
       " 'twist',\n",
       " 'development',\n",
       " 'girlfriend',\n",
       " 'pay',\n",
       " 'telling',\n",
       " 'write',\n",
       " 'rich',\n",
       " 'superb',\n",
       " 'various',\n",
       " 'meets',\n",
       " 'c',\n",
       " 'unique',\n",
       " 'dance',\n",
       " 'weird',\n",
       " 'island',\n",
       " 'william',\n",
       " 'directors',\n",
       " 'plenty',\n",
       " 'secret',\n",
       " 'break',\n",
       " 'fighting',\n",
       " 'disney',\n",
       " 'front',\n",
       " 'apart',\n",
       " 'brings',\n",
       " 'sounds',\n",
       " 'masterpiece',\n",
       " 'doctor',\n",
       " 'fairly',\n",
       " 'incredibly',\n",
       " 'outside',\n",
       " 'villain',\n",
       " 'dream',\n",
       " 'married',\n",
       " 'missing',\n",
       " 'leading',\n",
       " 'party',\n",
       " 'manages',\n",
       " 'return',\n",
       " 'beauty',\n",
       " 'remake',\n",
       " 'reasons',\n",
       " 'inside',\n",
       " 'zombie',\n",
       " 'fantasy',\n",
       " 'list',\n",
       " 'admit',\n",
       " 'rate',\n",
       " 'political',\n",
       " 'ideas',\n",
       " 'create',\n",
       " 'creepy',\n",
       " 'ask',\n",
       " 'meant',\n",
       " 'joke',\n",
       " 'unlike',\n",
       " 'potential',\n",
       " 'cute',\n",
       " 'dumb',\n",
       " 'success',\n",
       " 'copy',\n",
       " 'portrayed',\n",
       " 'nudity',\n",
       " 'fails',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "model.wv.index2word\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    \n",
    "#model = Word2Vec.load(\"300features_40minwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype = \"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for i in words:\n",
    "        if i in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[i])\n",
    "            \n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype = \"float32\")\n",
    "    \n",
    "    for j in reviews:\n",
    "        if counter % 5000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(j, model, num_features)\n",
    "        \n",
    "        counter  = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eve/anaconda2/envs/nlp3/lib/python3.4/site-packages/ipykernel/__main__.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 0 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for i in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordlist(i, remove_stopwords = True))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, 300)\n",
    "\n",
    "\n",
    "clean_test_reviews = []\n",
    "for j in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordlist(j, remove_stopwords = True))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "(25000, 300)\n",
      "[[ 0.01300876  0.01047531  0.00367966 ..., -0.0239167  -0.00254921\n",
      "  -0.00640002]\n",
      " [ 0.04700413  0.00239555 -0.02017518 ..., -0.01232704 -0.00812424\n",
      "  -0.00573464]\n",
      " [-0.01612295  0.00396236  0.01739677 ..., -0.02867449  0.01314587\n",
      "  -0.01016296]\n",
      " ..., \n",
      " [ 0.0123734   0.00477476 -0.00576705 ..., -0.00718576 -0.01778827\n",
      "   0.00567526]\n",
      " [ 0.02530028  0.00200104 -0.02043257 ..., -0.02031976  0.01739063\n",
      "   0.01097241]\n",
      " [ 0.01760322 -0.02194046 -0.01950093 ..., -0.00689489  0.00377015\n",
      "   0.01398756]]\n"
     ]
    }
   ],
   "source": [
    "print(trainDataVecs.shape)\n",
    "print(testDataVecs.shape)\n",
    "print(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])\n",
    "result = forest.predict(testDataVecs)\n",
    "\n",
    "output = pd.DataFrame(data = {\"id\":test[\"id\"], \"sentiment\": result})\n",
    "output.to_csv(\"Word2Vec_trial1.csv\", index = False, quoting = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  sentiment\n",
       "0  \"12311_10\"          1\n",
       "1    \"8348_2\"          0\n",
       "2    \"5828_4\"          1\n",
       "3    \"7186_2\"          0\n",
       "4   \"12128_7\"          1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
